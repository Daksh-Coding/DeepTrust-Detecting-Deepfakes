{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ab1c38c",
   "metadata": {},
   "source": [
    "# Importing Libraries and bringing data in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "725e8b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DAKSH KOTHARI\\anaconda3\\envs\\deepfake_env\\Lib\\site-packages\\h5py\\__init__.py:36: UserWarning: h5py is running against HDF5 1.14.6 when it was built against 1.14.5, this may cause problems\n",
      "  _warn((\"h5py is running against HDF5 {0} when it was built against {1}, \"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from mtcnn import MTCNN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "\n",
    "base_dir = \"mini-face-forensics/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31676c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fake', 'real']\n"
     ]
    }
   ],
   "source": [
    "labels = os.listdir(base_dir)\n",
    "print (labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6c4e6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = MTCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f840c577",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_paths = []\n",
    "labels = []  \n",
    "for sub_folder in os.listdir(base_dir):\n",
    "    label = sub_folder\n",
    "    sub_folder = os.path.join(base_dir, sub_folder)\n",
    "    for vid in os.listdir(sub_folder):\n",
    "        video_paths.append(os.path.join(sub_folder, vid))\n",
    "        labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2927ce0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 video paths: ['mini-face-forensics/fake\\\\01_02__outside_talking_still_laughing__YVGY8LOK.mp4', 'mini-face-forensics/fake\\\\01_02__walk_down_hall_angry__YVGY8LOK.mp4', 'mini-face-forensics/fake\\\\01_03__hugging_happy__ISF9SP4G.mp4', 'mini-face-forensics/fake\\\\01_03__podium_speech_happy__480LQD1C.mp4', 'mini-face-forensics/fake\\\\01_03__talking_against_wall__JZUXXFRB.mp4', 'mini-face-forensics/fake\\\\01_11__meeting_serious__9OM3VE0Y.mp4', 'mini-face-forensics/fake\\\\01_11__secret_conversation__4OJNJLOO.mp4', 'mini-face-forensics/fake\\\\01_11__talking_against_wall__9229VVZ3.mp4', 'mini-face-forensics/fake\\\\01_11__walking_outside_cafe_disgusted__FAFWDR4W.mp4', 'mini-face-forensics/fake\\\\01_12__outside_talking_pan_laughing__TNI7KUZ6.mp4']\n",
      "Last 10 video paths: ['mini-face-forensics/real\\\\15__outside_talking_still_laughing.mp4', 'mini-face-forensics/real\\\\15__podium_speech_happy.mp4', 'mini-face-forensics/real\\\\15__talking_against_wall.mp4', 'mini-face-forensics/real\\\\15__talking_angry_couch.mp4', 'mini-face-forensics/real\\\\15__walking_and_outside_surprised.mp4', 'mini-face-forensics/real\\\\15__walking_down_indoor_hall_disgust.mp4', 'mini-face-forensics/real\\\\15__walking_down_street_outside_angry.mp4', 'mini-face-forensics/real\\\\15__walking_outside_cafe_disgusted.mp4', 'mini-face-forensics/real\\\\15__walk_down_hall_angry.mp4', 'mini-face-forensics/real\\\\16__exit_phone_room.mp4']\n",
      "First 10 labels: ['fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake']\n",
      "Last 10 labels: ['real', 'real', 'real', 'real', 'real', 'real', 'real', 'real', 'real', 'real']\n"
     ]
    }
   ],
   "source": [
    "print(\"First 10 video paths:\", video_paths[:10])\n",
    "print(\"Last 10 video paths:\", video_paths[-10:])\n",
    "print(\"First 10 labels:\", labels[:10])\n",
    "print(\"Last 10 labels:\", labels[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef1e9bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          video_path label\n",
      "0  mini-face-forensics/fake\\01_02__outside_talkin...  fake\n",
      "1  mini-face-forensics/fake\\01_02__walk_down_hall...  fake\n",
      "2  mini-face-forensics/fake\\01_03__hugging_happy_...  fake\n",
      "3  mini-face-forensics/fake\\01_03__podium_speech_...  fake\n",
      "4  mini-face-forensics/fake\\01_03__talking_agains...  fake\n",
      "                                            video_path label\n",
      "395  mini-face-forensics/real\\15__walking_down_indo...  real\n",
      "396  mini-face-forensics/real\\15__walking_down_stre...  real\n",
      "397  mini-face-forensics/real\\15__walking_outside_c...  real\n",
      "398  mini-face-forensics/real\\15__walk_down_hall_an...  real\n",
      "399   mini-face-forensics/real\\16__exit_phone_room.mp4  real\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({'video_path': video_paths, 'label': labels})\n",
    "print(df.head())\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c00fa9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f71a5da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total videos for training: 320\n",
      "Total videos for testing: 80\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total videos for training: {len(train_df)}\")\n",
    "print(f\"Total videos for testing: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5f9efae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_center(frame):\n",
    "    y, x = frame.shape[0:2]\n",
    "    min_dim = min(y, x)\n",
    "    start_x = (x//2)-(min_dim//2)\n",
    "    start_y = (y//2)-(min_dim//2)\n",
    "    return frame[start_y:start_y+min_dim, start_x:start_x+min_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76813275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return frame and face bounding box for first frame, and then reusing same box to save computation of detector\n",
    "def get_face_region_for_first_frame(frame, previous_box=None):\n",
    "    if previous_box is None:\n",
    "        # Detect the face only if no previous bounding box is provided\n",
    "        detections = detector.detect_faces(frame)\n",
    "        if detections:\n",
    "            x, y, w, h = detections[0]['box']\n",
    "            previous_box = (x, y, w, h)\n",
    "        else:\n",
    "            return get_center(frame), None  # fallback to center crop if no face detected\n",
    "    else:\n",
    "        x, y, w, h = previous_box\n",
    "\n",
    "    face_region = frame[y:y+h, x:x+w]\n",
    "    return face_region, previous_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c372148",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE), skip_frames=2):\n",
    "    cap = cv.VideoCapture(path)\n",
    "    frames = []\n",
    "    frame_count = 0\n",
    "    previous_box = None  # Store the bounding box from first frame\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            # Process frames as per skip rate to avoid getting near same frames\n",
    "            if frame_count % skip_frames == 0:\n",
    "                frame, previous_box = get_face_region_for_first_frame(frame, previous_box)\n",
    "                frame = cv.resize(frame, resize)\n",
    "                frame = frame[:, :, [2, 1, 0]]  # BGR to RGB\n",
    "                frames.append(frame)\n",
    "                if len(frames) == max_frames:\n",
    "                    break\n",
    "            frame_count += 1\n",
    "        # Pad with the last frame if we have fewer frames than max_frames\n",
    "        while len(frames) < max_frames and frames:\n",
    "            frames.append(frames[-1])\n",
    "    finally:\n",
    "        cap.release()\n",
    "    return np.array(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173cb116",
   "metadata": {},
   "source": [
    "# Building Feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c1a3ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_extractor():\n",
    "    feature_extractor = keras.applications.InceptionV3(\n",
    "        weights=\"imagenet\",\n",
    "        include_top=False,\n",
    "        pooling=\"avg\",\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 3),)\n",
    "    preprocess_input = keras.applications.inception_v3.preprocess_input\n",
    "    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
    "    preprocessed = preprocess_input(inputs)\n",
    "    outputs = feature_extractor(preprocessed)\n",
    "    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
    "\n",
    "feature_extractor = build_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcdefc97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fake', 'real']\n"
     ]
    }
   ],
   "source": [
    "labeler = keras.layers.StringLookup(num_oov_indices=0, vocabulary=np.unique(train_df[\"label\"]))\n",
    "print(labeler.get_vocabulary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7694dd0e",
   "metadata": {},
   "source": [
    "# Extracting frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5b961db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "MAX_SEQ_LENGTH = 20\n",
    "NUM_FEATURES = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73de87c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 320/320 [47:50<00:00,  8.97s/it]  \n",
      "Extracting features: 100%|██████████| 80/80 [10:31<00:00,  7.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame features in train set: (320, 20, 2048)\n",
      "Frame masks in train set: (320, 20)\n",
      "train_labels in train set: (320, 1)\n",
      "test_labels in train set: (80, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def prepare_all_videos(df):\n",
    "    num_samples = len(df)\n",
    "    video_paths = df[\"video_path\"].values.tolist()\n",
    "    labels = df[\"label\"].values\n",
    "    #convert classlabels to integer format(one-hot)\n",
    "    labels = labeler(labels[..., None]).numpy()\n",
    "    # `frame_masks` and `frame_features` are what we will feed to our sequence model.\n",
    "    # `frame_masks` will contain a bunch of booleans denoting if a timestep is masked with padding or not.\n",
    "    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype=\"bool\") # 320,20\n",
    "    frame_features = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\") #320,20,2048\n",
    "    # For each video.\n",
    "    for idx, path in enumerate(tqdm(video_paths, desc=\"Extracting features\")):\n",
    "        # Gather all its frames and add a batch dimension.\n",
    "        frames = load_video(path)\n",
    "        frames = frames[None, ...]\n",
    "        # Initialize placeholders to store the masks and features of the current video.\n",
    "        temp_frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "        temp_frame_features = np.zeros(shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n",
    "        # Extract features from the frames of the current video.\n",
    "        for i, batch in enumerate(frames):\n",
    "            video_length = batch.shape[0]\n",
    "            length = min(MAX_SEQ_LENGTH, video_length)\n",
    "            for j in range(length):\n",
    "                temp_frame_features[i, j, :] = feature_extractor.predict(batch[None, j, :], verbose=0)\n",
    "            temp_frame_mask[i, :length] = 1  # 1=not masked, 0=masked\n",
    "\n",
    "        frame_features[idx,] = temp_frame_features.squeeze()\n",
    "        frame_masks[idx,] = temp_frame_mask.squeeze()\n",
    "\n",
    "    return (frame_features, frame_masks), labels\n",
    "\n",
    "train_data, train_labels = prepare_all_videos(train_df)\n",
    "test_data, test_labels = prepare_all_videos(test_df)\n",
    "print(f\"Frame features in train set: {train_data[0].shape}\")\n",
    "print(f\"Frame masks in train set: {train_data[1].shape}\")\n",
    "print(f\"train_labels in train set: {train_labels.shape}\")\n",
    "print(f\"test_labels in train set: {test_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d741a1",
   "metadata": {},
   "source": [
    "# Creating LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c624692a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DAKSH KOTHARI\\anaconda3\\envs\\deepfake_env\\Lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor_334', 'keras_tensor_335']. Received: the structure of inputs=('*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.4301 - loss: 0.7251\n",
      "Epoch 1: val_loss improved from inf to 0.70529, saving model to deepfake_detector_model.keras\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 77ms/step - accuracy: 0.4561 - loss: 0.7148 - val_accuracy: 0.4792 - val_loss: 0.7053\n",
      "Epoch 2/30\n",
      "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4960 - loss: 0.7356\n",
      "Epoch 2: val_loss improved from 0.70529 to 0.68493, saving model to deepfake_detector_model.keras\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.4981 - loss: 0.7290 - val_accuracy: 0.5000 - val_loss: 0.6849\n",
      "Epoch 3/30\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5584 - loss: 0.6733\n",
      "Epoch 3: val_loss improved from 0.68493 to 0.67015, saving model to deepfake_detector_model.keras\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.5628 - loss: 0.6731 - val_accuracy: 0.6562 - val_loss: 0.6702\n",
      "Epoch 4/30\n",
      "\u001b[1m5/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5776 - loss: 0.6790\n",
      "Epoch 4: val_loss did not improve from 0.67015\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5901 - loss: 0.6752 - val_accuracy: 0.6042 - val_loss: 0.6743\n",
      "Epoch 5/30\n",
      "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6768 - loss: 0.6437\n",
      "Epoch 5: val_loss improved from 0.67015 to 0.65209, saving model to deepfake_detector_model.keras\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6750 - loss: 0.6426 - val_accuracy: 0.6354 - val_loss: 0.6521\n",
      "Epoch 6/30\n",
      "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6951 - loss: 0.6463\n",
      "Epoch 6: val_loss did not improve from 0.65209\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6976 - loss: 0.6412 - val_accuracy: 0.5938 - val_loss: 0.6785\n",
      "Epoch 7/30\n",
      "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6463 - loss: 0.6504\n",
      "Epoch 7: val_loss improved from 0.65209 to 0.62765, saving model to deepfake_detector_model.keras\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6532 - loss: 0.6467 - val_accuracy: 0.6875 - val_loss: 0.6276\n",
      "Epoch 8/30\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6972 - loss: 0.5957 \n",
      "Epoch 8: val_loss improved from 0.62765 to 0.59684, saving model to deepfake_detector_model.keras\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6999 - loss: 0.5945 - val_accuracy: 0.7083 - val_loss: 0.5968\n",
      "Epoch 9/30\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8233 - loss: 0.5464\n",
      "Epoch 9: val_loss improved from 0.59684 to 0.57495, saving model to deepfake_detector_model.keras\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8253 - loss: 0.5449 - val_accuracy: 0.7500 - val_loss: 0.5749\n",
      "Epoch 10/30\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7987 - loss: 0.5431 \n",
      "Epoch 10: val_loss improved from 0.57495 to 0.56427, saving model to deepfake_detector_model.keras\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7926 - loss: 0.5456 - val_accuracy: 0.7083 - val_loss: 0.5643\n",
      "Epoch 11/30\n",
      "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7737 - loss: 0.4796\n",
      "Epoch 11: val_loss improved from 0.56427 to 0.54401, saving model to deepfake_detector_model.keras\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7778 - loss: 0.4822 - val_accuracy: 0.7500 - val_loss: 0.5440\n",
      "Epoch 12/30\n",
      "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8231 - loss: 0.4381\n",
      "Epoch 12: val_loss improved from 0.54401 to 0.53113, saving model to deepfake_detector_model.keras\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.8216 - loss: 0.4452 - val_accuracy: 0.7708 - val_loss: 0.5311\n",
      "Epoch 13/30\n",
      "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7936 - loss: 0.4909\n",
      "Epoch 13: val_loss did not improve from 0.53113\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7961 - loss: 0.4897 - val_accuracy: 0.7917 - val_loss: 0.5396\n",
      "Epoch 14/30\n",
      "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7725 - loss: 0.4513\n",
      "Epoch 14: val_loss did not improve from 0.53113\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7825 - loss: 0.4491 - val_accuracy: 0.7188 - val_loss: 0.5876\n",
      "Epoch 15/30\n",
      "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7935 - loss: 0.5165\n",
      "Epoch 15: val_loss improved from 0.53113 to 0.50552, saving model to deepfake_detector_model.keras\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7982 - loss: 0.5069 - val_accuracy: 0.7812 - val_loss: 0.5055\n",
      "Epoch 16/30\n",
      "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8046 - loss: 0.5230\n",
      "Epoch 16: val_loss did not improve from 0.50552\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.8133 - loss: 0.5047 - val_accuracy: 0.7812 - val_loss: 0.5349\n",
      "Epoch 17/30\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8472 - loss: 0.3853\n",
      "Epoch 17: val_loss improved from 0.50552 to 0.46326, saving model to deepfake_detector_model.keras\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8456 - loss: 0.3852 - val_accuracy: 0.8021 - val_loss: 0.4633\n",
      "Epoch 18/30\n",
      "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8337 - loss: 0.3960\n",
      "Epoch 18: val_loss did not improve from 0.46326\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8407 - loss: 0.3921 - val_accuracy: 0.7083 - val_loss: 0.5908\n",
      "Epoch 19/30\n",
      "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8872 - loss: 0.3218\n",
      "Epoch 19: val_loss did not improve from 0.46326\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8842 - loss: 0.3276 - val_accuracy: 0.7292 - val_loss: 0.5977\n",
      "Epoch 20/30\n",
      "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9183 - loss: 0.3035\n",
      "Epoch 20: val_loss did not improve from 0.46326\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9142 - loss: 0.3065 - val_accuracy: 0.7396 - val_loss: 0.5592\n",
      "Epoch 21/30\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9097 - loss: 0.2933 \n",
      "Epoch 21: val_loss did not improve from 0.46326\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9098 - loss: 0.2933 - val_accuracy: 0.8021 - val_loss: 0.4843\n",
      "Epoch 22/30\n",
      "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9064 - loss: 0.2828\n",
      "Epoch 22: val_loss did not improve from 0.46326\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9075 - loss: 0.2804 - val_accuracy: 0.7292 - val_loss: 0.5767\n",
      "Epoch 23/30\n",
      "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9265 - loss: 0.2496\n",
      "Epoch 23: val_loss did not improve from 0.46326\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9203 - loss: 0.2633 - val_accuracy: 0.7292 - val_loss: 0.6188\n",
      "Epoch 24/30\n",
      "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9232 - loss: 0.2549\n",
      "Epoch 24: val_loss did not improve from 0.46326\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9201 - loss: 0.2612 - val_accuracy: 0.8021 - val_loss: 0.5103\n",
      "Epoch 25/30\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9418 - loss: 0.2232\n",
      "Epoch 25: val_loss did not improve from 0.46326\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9374 - loss: 0.2276 - val_accuracy: 0.7292 - val_loss: 0.5956\n",
      "Epoch 26/30\n",
      "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9369 - loss: 0.2455\n",
      "Epoch 26: val_loss did not improve from 0.46326\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9315 - loss: 0.2539 - val_accuracy: 0.7396 - val_loss: 0.5864\n",
      "Epoch 27/30\n",
      "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8887 - loss: 0.3276\n",
      "Epoch 27: val_loss did not improve from 0.46326\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8931 - loss: 0.3136 - val_accuracy: 0.7604 - val_loss: 0.5624\n",
      "Epoch 28/30\n",
      "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9533 - loss: 0.1893\n",
      "Epoch 28: val_loss did not improve from 0.46326\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9438 - loss: 0.2020 - val_accuracy: 0.8125 - val_loss: 0.4822\n",
      "Epoch 29/30\n",
      "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9015 - loss: 0.2196\n",
      "Epoch 29: val_loss did not improve from 0.46326\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9049 - loss: 0.2223 - val_accuracy: 0.7292 - val_loss: 0.6748\n",
      "Epoch 30/30\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9434 - loss: 0.1853\n",
      "Epoch 30: val_loss did not improve from 0.46326\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9438 - loss: 0.1854 - val_accuracy: 0.7604 - val_loss: 0.6147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DAKSH KOTHARI\\anaconda3\\envs\\deepfake_env\\Lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['input_layer_4', 'input_layer_5']. Received: the structure of inputs=('*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8211 - loss: 0.4340  \n",
      "Test accuracy: 83.75%\n"
     ]
    }
   ],
   "source": [
    "def build_LSTM_model():\n",
    "    class_vocab = labeler.get_vocabulary()\n",
    "    frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))\n",
    "    mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "    # to learn about masking, see keras api documentation\n",
    "    x = keras.layers.LSTM(16, return_sequences=True)(frame_features_input, mask=mask_input)\n",
    "    x = keras.layers.LSTM(8)(x)\n",
    "    x = keras.layers.Dropout(0.4)(x)\n",
    "    x = keras.layers.Dense(8, activation=\"relu\")(x)\n",
    "    output = keras.layers.Dense(len(class_vocab), activation=\"softmax\")(x)\n",
    "    lstm_model = keras.Model([frame_features_input, mask_input], output)\n",
    "    lstm_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    return lstm_model\n",
    "\n",
    "def run_experiment():\n",
    "    filepath = \"deepfake_detector_model.keras\"\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(filepath, save_best_only=True, verbose=1)\n",
    "    lstm_model = build_LSTM_model()\n",
    "    history = lstm_model.fit([train_data[0], train_data[1]],train_labels,validation_split=0.3,epochs=30,callbacks=[checkpoint],)\n",
    "\n",
    "    lstm_model=keras.models.load_model(filepath)\n",
    "    _, accuracy = lstm_model.evaluate([test_data[0], test_data[1]], test_labels)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    return history, lstm_model\n",
    "\n",
    "_, lstm_model = run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "06fcc243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test video path: mini-face-forensics/real\\04__talking_against_wall.mp4\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "  real: 86.61%\n",
      "  fake: 13.39%\n"
     ]
    }
   ],
   "source": [
    "def prepare_single_video(frames):\n",
    "    frames = frames[None, ...]\n",
    "    frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "    frame_features = np.zeros(shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n",
    "    for i, batch in enumerate(frames):\n",
    "        video_length = batch.shape[0]\n",
    "        length = min(MAX_SEQ_LENGTH, video_length)\n",
    "        for j in range(length):\n",
    "            frame_features[i, j, :] = feature_extractor.predict(batch[None, j, :],verbose=0)\n",
    "        frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
    "\n",
    "    return frame_features, frame_mask\n",
    "\n",
    "\n",
    "def sequence_prediction(path):\n",
    "    class_vocab = labeler.get_vocabulary()\n",
    "    frames = load_video(path)\n",
    "    frame_features, frame_mask = prepare_single_video(frames)\n",
    "    probabilities = lstm_model.predict([frame_features, frame_mask])[0]\n",
    "    for i in np.argsort(probabilities)[::-1]:\n",
    "        print(f\"  {class_vocab[i]}: {probabilities[i] * 100:5.2f}%\")\n",
    "    return frames\n",
    "\n",
    "test_video = np.random.choice(test_df[\"video_path\"].values.tolist())\n",
    "print(f\"Test video path: {test_video}\")\n",
    "test_frames = sequence_prediction(test_video)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c19a35bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "  fake: 85.68%\n",
      "  real: 14.32%\n"
     ]
    }
   ],
   "source": [
    "video=\"pk_screening_aamir_khan_deepfake.mp4\"\n",
    "sampled_frames=sequence_prediction(video)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepfake_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
